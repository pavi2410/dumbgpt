# DumbGPT

A **Mini-GPT implementation from scratch** in TypeScript, designed for learning how transformers work.

## Features

ğŸ§  **Complete GPT Implementation**
- Multi-head attention mechanism
- Transformer blocks with residual connections
- Layer normalization and GELU activation
- Custom tensor operations (no external ML libraries)
- BPE-style tokenizer for JavaScript/TypeScript

ğŸ¯ **Code-Focused Training**
- Trains on JavaScript/TypeScript files from `node_modules`
- Generates contextually aware code completions
- Supports various JavaScript patterns and syntax

ğŸ–¥ï¸ **Modern TUI Interface**
- Beautiful terminal interface built with Ink
- Real-time training progress visualization
- Command system for configuration
- Input history with arrow key navigation

## Installation

```bash
bun install
```

## Usage

```bash
# Start the application
bun run start

# Development mode (with file watching)
bun run dev

# Run tests
bun test
```

## Commands

- `:help` - Show help
- `:config` - Show configuration  
- `:set <key> <value>` - Change settings
- `:clear` - Clear messages
- `:quit` - Exit

## Configuration

- `contextSize` - Context window size (default: 32)
- `maxOutputTokens` - Maximum tokens to generate (default: 50)
- `modelType` - Model type: 'code' or 'text' (default: 'code')

## Project Structure

```
src/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ gpt/
â”‚   â”‚   â”œâ”€â”€ tensor.ts        # Custom tensor operations
â”‚   â”‚   â”œâ”€â”€ tokenizer.ts     # BPE tokenizer for code
â”‚   â”‚   â”œâ”€â”€ attention.ts     # Multi-head attention
â”‚   â”‚   â””â”€â”€ transformer.ts   # Complete GPT model
â”‚   â”œâ”€â”€ gpt-model.ts         # GPT model integration
â”‚   â””â”€â”€ index.ts             # Models API
â”œâ”€â”€ ui/
â”‚   â”œâ”€â”€ hooks/
â”‚   â”‚   â””â”€â”€ useInputHistory.ts  # Input handling hooks
â”‚   â”œâ”€â”€ App.tsx              # Main application
â”‚   â”œâ”€â”€ ChatArea.tsx         # Chat interface
â”‚   â”œâ”€â”€ InputArea.tsx        # Input with history
â”‚   â”œâ”€â”€ TrainingProgress.tsx # Training visualization
â”‚   â””â”€â”€ theme.ts             # UI theme
â””â”€â”€ tui.tsx                  # Application entry point
```

## Model Architecture

**Mini-GPT Configuration:**
- **Parameters**: ~650K (much smaller than production models)
- **Layers**: 4 transformer blocks
- **Heads**: 8 attention heads  
- **Embedding**: 256 dimensions
- **Vocabulary**: 8000 tokens
- **Context**: 32 tokens

## Learning Objectives

This implementation demonstrates:
- How transformers work from first principles
- Multi-head attention mechanism
- Tokenization for code
- Training loop and loss calculation
- Tensor operations without external libraries

## Performance

- **Training**: CPU-only (no GPU required)
- **Speed**: ~600-800ms for full test suite
- **Memory**: Reasonable for development machines
- **Quality**: Generates coherent code tokens

## Testing

Comprehensive test suite covering:
- Tensor operations (matrix multiplication, softmax, etc.)
- Tokenization (encode/decode, training sequences)
- Attention mechanism
- Complete model training and generation

```bash
bun test  # Run all tests
```

## Next Steps

Potential improvements:
1. Implement proper backpropagation
2. Add beam search for better generation
3. Expand training data
4. Add fine-tuning capabilities
5. Implement more sophisticated attention patterns

---

**Note**: This is an educational implementation focused on understanding transformers. For production use, consider established libraries like transformers.js or TensorFlow.js.
